{"cells":[{"cell_type":"markdown","source":["##Image Processing Notebook\n\n###1. Process Description\n\nWe design our process to leverage the parralel processing power from Spark while ensuring we can train multiple models in parallel. One option we decided on was to transform the images using pyspark and save those transformed data into parquet files. Doing this it will allow us to load this dataset on other models in parralel and reduce the duplication of image transformation.\n\nFirst, we needed load the needed libraries. In this case, we need PIL and torchvision for imaging transformation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60a3885b-685a-490b-a7ed-c7ec9f4a76bb"}}},{"cell_type":"code","source":["#Add needed packages\nfrom pyspark.sql.functions import *\nimport numpy as np\nimport pandas as pd\nfrom pyspark import SparkContext\nimport random\nfrom pyspark.sql.types import *\nfrom PIL import Image\nfrom torchvision import transforms\nfrom pyspark.sql.window import Window"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6283687-1ebd-4d19-8640-e33589a594c5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We use Azure Data Lake Store for storing all the images and transformed dataset. This mounting section allow us to mount to the storage account using Storage Key. Different mounting points point to different containers\n\n- **/mnt/files**: mounting alias for pictures\n- **/mnt/dim**: excel dataset about the patients\n- **/mnt/datasets**: transformed dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d33c348-0674-4189-817f-ca13ccb9abb9"}}},{"cell_type":"code","source":["#mount blob container to Databricks\ntry:\n  dbutils.fs.mount(\n    source = \"wasbs://images@chestxraystorage1.blob.core.windows.net\",\n    mount_point = \"/mnt/files\",\n    extra_configs = {\"fs.azure.account.key.chestxraystorage1.blob.core.windows.net\":\"VtYP71BSCTyhUtCeGkMSmRRnAvTPVC3v4RxDy9sQEBOdtWCWF2BWp4lsEm0iSkLI1/pmfKO3GrjWVfMCjTD1MA==\"})\nexcept:\n  print('Already mounted')\n  \n  \n#mount dimension container \ntry:\n  dbutils.fs.mount(\n    source = \"wasbs://dimension@chestxraystorage1.blob.core.windows.net\",\n    mount_point = \"/mnt/dim\",\n    extra_configs = {\"fs.azure.account.key.chestxraystorage1.blob.core.windows.net\":\"VtYP71BSCTyhUtCeGkMSmRRnAvTPVC3v4RxDy9sQEBOdtWCWF2BWp4lsEm0iSkLI1/pmfKO3GrjWVfMCjTD1MA==\"})\nexcept:\n  print('Already mounted')\n  \n  \n#mount datasets container \ntry:\n  dbutils.fs.mount(\n    source = \"wasbs://datasets@chestxraystorage1.blob.core.windows.net\",\n    mount_point = \"/mnt/datasets\",\n    extra_configs = {\"fs.azure.account.key.chestxraystorage1.blob.core.windows.net\":\"VtYP71BSCTyhUtCeGkMSmRRnAvTPVC3v4RxDy9sQEBOdtWCWF2BWp4lsEm0iSkLI1/pmfKO3GrjWVfMCjTD1MA==\"})\nexcept:\n  print('Already mounted')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4fab8d85-0a0b-4a51-ba4b-7b390bf0bdb1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 2. Transform the Patient Demography\n\nWe need to transform the output deseases into 14 output binary columns. Each column represent a desease. The output of this pyspark dataframe is a patient dimension with the Image Index (name) is the key column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8102b44-9f3c-42f3-8d9c-b7c5f81be792"}}},{"cell_type":"code","source":["#clean dimension response. '\n\ndf_response = spark.read.csv(\"/mnt/dim/Data_Entry_2017_v2020.csv\",header=True)\ndf_response = df_response.select(col('Image Index').alias('name'),\n                        col('Finding Labels').alias('label'),\n                        col('Follow-up #').alias('followUp'),\n                        col('Patient ID').alias('pId'),\n                        col('Patient Age').alias('Age'),\n                        col('Patient Gender').alias('Gender'),\n                        col('View Position').alias('Position'),\n                        col('OriginalImage[Width').alias('W'),\n                        col('Height]').alias('H'),\n                        col('OriginalImagePixelSpacing[x').alias('x'),\n                        col('y]').alias('y'))\n\n#pass label into response variables\n\nlabels = {'y0':'Atelectasis',\n          'y1':'Cardiomegaly',\n          'y2':'Consolidation',\n          'y3':'Edema',\n          'y4':'Effusion',\n          'y5':'Emphysema',\n          'y6':'Fibrosis',\n          'y7':'Hernia',\n          'y8':'Infiltration',\n          'y9':'Mass',\n          'y10':'Nodule',\n          'y11':'Pleural_Thickening',\n          'y12':'Pneumonia',\n          'y13':'Pneumothorax'} #this may be used after training\n\ndf_response = df_response.withColumn('y0',when(col('label').contains('Atelectasis'),lit(1)).otherwise(lit(0))) \\\n                          .withColumn('y1',when(col('label').contains('Cardiomegaly'),lit(1)).otherwise(lit(0))) \\\n                          .withColumn('y2',when(col('label').contains('Consolidation'),lit(1)).otherwise(lit(0))) \\\n                          .withColumn('y3',when(col('label').contains('Edema'),lit(1)).otherwise(lit(0))) \\\n                          .withColumn('y4',when(col('label').contains('Effusion'),lit(1)).otherwise(lit(0))) \\\n                          .withColumn('y5',when(col('label').contains('Emphysema'),lit(1)).otherwise(lit(0))) \\\n                          .withColumn('y6',when(col('label').contains('Fibrosis'),lit(1)).otherwise(lit(0))) \\\n                          .withColumn('y7',when(col('label').contains('Hernia'),lit(1)).otherwise(lit(0))) \\\n                          .withColumn('y8',when(col('label').contains('Infiltration'),lit(1)).otherwise(lit(0))) \\\n                          .withColumn('y9',when(col('label').contains('Mass'),lit(1)).otherwise(lit(0))) \\\n                          .withColumn('y10',when(col('label').contains('Nodule'),lit(1)).otherwise(lit(0))) \\\n                          .withColumn('y11',when(col('label').contains('Pleural_Thickening'),lit(1)).otherwise(lit(0))) \\\n                          .withColumn('y12',when(col('label').contains('Pneumonia'),lit(1)).otherwise(lit(0))) \\\n                          .withColumn('y13',when(col('label').contains('Pneumothorax'),lit(1)).otherwise(lit(0)))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d79cd00-3a5e-4330-b822-93cdb0902b53"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 3. Preparing train, validation, and test datasets\n\nWith over 112,000 images, we found that data splitting process is crucial. For example, if we split by randomzied images, it would creating bias since the images of the same patients may appear in train, validation and test dataset (e.g. think about a patient with 3 images of Hernia positive and each may go to each dataset, it's like training and validating on the same patient). As such, we have to do random split based on patient instead of images. \n\nIn addition, due to limitation of memory, we split our train and test dataset of patients into 20 partitions (14 for train and 6 for validation) with seed of 2510. The same thing happen for test patients with 7 partitions. \n\nAt the end, we created a three lists that contain coresponding partitions (e.g. 14 train partions will go to Train list and 7 test partitions will go to Test list). This would help us easily loop through each partition and do image transformation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f52172f-c54a-445b-aa8c-81d4d51f87a0"}}},{"cell_type":"code","source":["filesDF = spark.read.format(\"csv\").load('/mnt/dim/filesDF.csv',header=True)\n\nseed=2510\n\ndf_trainvalid = spark.read.format(\"text\").load(\"/mnt/dim/train_val_list.txt\")\ndf_test = spark.read.format(\"text\").load(\"/mnt/dim/test_list.txt\")\n\ndf_pId = df_response.select('name','pId').distinct()\ndf_trainvalid = df_trainvalid.join(df_pId, df_trainvalid.value == df_pId.name, 'inner')\ndf_test = df_test.join(df_pId, df_test.value == df_pId.name, 'inner')\n\n#---------------------\ndf_trainvalidpId = df_trainvalid.select('pId').distinct().orderBy(rand(seed))\n\ndf_trainpId1, df_trainpId2, df_trainpId3, df_trainpId4, df_trainpId5, df_trainpId6, df_trainpId7, df_trainpId8, df_trainpId9, df_trainpId10, df_trainpId11, df_trainpId12, df_trainpId13, df_trainpId14, df_validpId1, df_validpId2, df_validpId3, df_validpId4, df_validpId5, df_validpId6  = df_trainvalidpId.randomSplit([0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05, 0.05, 0.05, 0.05, 0.05, 0.05], seed=2510)\n\n#---------------------\ndf_testpId = df_test.select('pId').distinct()\ndf_testpId = df_testpId.orderBy(rand(seed))\n\ndf_testpId1, df_testpId2, df_testpId3, df_testpId4, df_testpId5, df_testpId6, df_testpId7 = df_testpId.randomSplit([0.15,0.15,0.15,0.15,0.15,0.15,0.10], seed=2510)\n\n#----------------------\n\ntrain = [df_trainpId1, df_trainpId2, df_trainpId3, df_trainpId4, df_trainpId5, df_trainpId6, df_trainpId7, df_trainpId8, df_trainpId9, df_trainpId10, df_trainpId11, df_trainpId12, df_trainpId13, df_trainpId14]\nvalid = [df_validpId1, df_validpId2, df_validpId3, df_validpId4, df_validpId5, df_validpId6]\ntest = [df_testpId1, df_testpId2, df_testpId3, df_testpId4, df_testpId5, df_testpId6, df_testpId7]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1256cf04-5704-46fb-8d3c-a4cc0d1d8282"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 4. Define the transformation function\n\nWe use Torchvision to transform the original images (1024x1024x4) into 512x512x3 and 256x256x3 format. In addition, we also normalized the image tensor using mean and standard diviation. The output of this UDF function is a flatten numpy array."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7f6ebd4-a216-4f90-ac48-eb28c673ef97"}}},{"cell_type":"code","source":["def img2ArrayPIL(name):\n  \n  try:\n    im = Image.open(\"/dbfs/mnt/files/\" + name)\n    im = im.convert(mode='RGB')\n    preprocess = transforms.Compose([\n      transforms.Resize((256,256)),\n      transforms.ToTensor(),\n      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n\n    output = preprocess(im).numpy().flatten()\n  except:\n    output = np.array(0)\n  \n  return output\n\nimg2ArrayUDF = udf(lambda z: img2ArrayPIL(z).tolist(),ArrayType(FloatType()))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"232abfe7-fd2c-49d2-bc02-e4ac19eeb3be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["####5. Define export function\n\nThe function would allow us call the transformation function and save the output into parquet files in 8 different partitions. When doing the repartition(8) method, we obversed the power of Spark. This parallel processing speed up the entire process and reduced the processing time by by at least 60%. In the past, we had struggle of exporting a 256x256x1 to csv and took around 90 minutes for 5000 images. With spark parralel processing, we can do up to 256x256x3 in less than 10 minutes for 5000 images."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c031640-238c-42a8-bfc5-0c605ee26516"}}},{"cell_type":"code","source":["def extract_features(df, i, dataset, originalDF):\n  resultDF = originalDF.join(df, df.pId == originalDF.pId, 'inner').drop('name').drop('pId').drop('Id')\n  resultDF = resultDF.join(filesDF, resultDF.value == filesDF.name, 'inner')\n  resultDF = resultDF.repartition(8).withColumn('pixels',img2ArrayUDF(filesDF.name))\n  dT = resultDF.select(col('name').alias('name1'),'pixels')\n  dT = dT.join(df_response, dT.name1 == df_response.name, 'inner') \\\n          .select('name','y0','y1','y2','y3','y4','y5','y6','y7','y8','y9','y10','y11','y12','y13','pixels')\n          #.withColumn('response',array('y0','y1','y2','y3','y4','y5','y6','y7','y8','y9','y10','y11','y12','y13')) \\\n  dT.repartition(8).write.mode(\"overwrite\").option(\"header\", \"true\").format(\"parquet\").save('/mnt/datasets/df'+dataset+'256_ALL_batchNo'+str(i))\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdd6a918-f299-46c3-9270-6fc8fb70c1a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["####6. Main Operation\n\nThis final for loops will go through each patient partition, transform the images and extract pixel features, then save the pyspark dataframe into storage account."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2c60864-76ca-441c-9b08-53154ca586f2"}}},{"cell_type":"code","source":["for i, df in enumerate(train):\n  extract_features(df, i, 'train', df_trainvalid)\n\nfor i, df in enumerate(valid):\n  extract_features(df, i, 'valid', df_trainvalid)\n  \nfor i, df in enumerate(test):\n  extract_features(df, i, 'test', df_test)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"824c7a69-1dd6-4309-9e4b-07e2a5dfd46e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ETL Process - Final","dashboards":[],"language":"python","widgets":{},"notebookOrigID":707438309562214}},"nbformat":4,"nbformat_minor":0}
